{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab=None, corpus=None):\n",
    "        self.vocab = vocab\n",
    "        if not vocab and corpus:\n",
    "            self.build_vocab(corpus=corpus, type='word')\n",
    "    \n",
    "    def build_vocab(self, corpus, type='word'):\n",
    "        if type == 'word':\n",
    "            result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', corpus)\n",
    "            self.vocab_type = 'word'\n",
    "        elif type == 'char':\n",
    "            result = list(corpus)\n",
    "            self.vocab_type = 'char'\n",
    "        vocab = sorted(set(token.strip() for token in result if token.strip()))\n",
    "        vocab.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "        self.vocab = vocab\n",
    "        self.ctoi = { c:i for i, c in enumerate(self.vocab)}\n",
    "        self.itoc = {i:c for c, i in self.ctoi.items()}\n",
    "        return vocab\n",
    "\n",
    "    def encode(self, text):\n",
    "        if self.vocab_type == 'word':\n",
    "            tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        elif self.vocab_type == 'char':\n",
    "            tokens = list(text)\n",
    "        cleaned_tokens = [token.strip() for token in tokens if token.strip()]\n",
    "        result = [self.ctoi[token] if token in self.vocab else self.ctoi[\"<|unk|>\"] for token in cleaned_tokens]\n",
    "        return result\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = \" \".join([self.itoc[token] for token in tokens])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "It' s the last he painted, you know, Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(corpus=raw_text)\n",
    "vocab = tokenizer.build_vocab(type='word', corpus=raw_text)\n",
    "tokens = tokenizer.encode(\"It's the last he painted, you know, Mrs. Gisburn said with pardonable pride.\")\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 1131, 0, 97, 584, 115, 1131, 7, 1131, 5, 1126, 5, 67, 7, 38]\n",
      "<|unk|>, <|unk|>! This is a <|unk|>. <|unk|>, you, Mrs. Gisburn\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"Hello, world! This is a test. Painted, you, Mrs. Gisburn\")\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 44, 149, 1003, 57, 38, 818, 115, 256, 486, 6, 1002, 115, 500, 435, 392, 6, 908, 585, 1077, 709, 508, 961, 1016, 663, 1016, 535, 987, 5, 568, 988, 538, 722, 549, 496, 5, 533, 514, 370, 549, 748, 5, 661, 115, 841, 1102, 5, 157, 397, 547, 568, 115, 1066, 727, 988, 84, 7, 3, 99, 53, 818, 1003, 585, 1120, 530, 208, 85, 734, 34, 7, 4, 1, 93, 538, 722, 549, 496, 1, 6, 987, 1077, 1089, 988, 1112, 242, 585, 7, 53, 244, 535, 67, 7, 37, 100, 6, 549, 602, 25, 897, 6, 326, 549, 1042, 116, 7, 1, 73, 297, 585, 2]\n",
      "I HAD always thought Jack Gisburn rather a cheap genius -- though a good fellow enough -- so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera.( Though I rather thought it would have been Rome or Florence.)\" The height of his glory\" -- that was what the women called it. I can hear Mrs. Gideon Thwing -- his last Chicago sitter -- deploring his unaccountable abdication.\" Of course it'\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(raw_text[:500])\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 983, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terrace of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terrace of the palace.\"\n",
    "text = \" <|endoftext|> \".join([text1, text2])\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 558, 286, 262, 20562, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terrace of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = (text)\n",
    "tokens = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "text = (\"Akwirw ier\")\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ak', 'w', 'ir', 'w', ' ', 'ier']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [tokenizer.decode([token]) for token in tokens]\n",
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
