{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmontgomery14/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 558, 286, 262, 20562, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terrace of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terrace of the palace.\"\n",
    "text = \" <|endoftext|> \".join([text1, text2])\n",
    "tokens = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "text = (\"Akwirw ier\")\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ak', 'w', 'ir', 'w', ' ', 'ier']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [tokenizer.decode([token]) for token in tokens]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "tokens = tokenizer.encode(raw_text) \n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = tokens[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    y = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = 10\n",
    "stride = 1\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "txt = raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5135, 10]) torch.Size([5135, 10])\n",
      "tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138],\n",
      "        [  367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257],\n",
      "        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n",
      "        [ 1464,  1807,  3619,   402,   271, 10899,  2138,   257,  7026, 15632],\n",
      "        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026, 15632,   438],\n",
      "        [ 3619,   402,   271, 10899,  2138,   257,  7026, 15632,   438,  2016],\n",
      "        [  402,   271, 10899,  2138,   257,  7026, 15632,   438,  2016,   257],\n",
      "        [  271, 10899,  2138,   257,  7026, 15632,   438,  2016,   257,   922],\n",
      "        [10899,  2138,   257,  7026, 15632,   438,  2016,   257,   922,  5891],\n",
      "        [ 2138,   257,  7026, 15632,   438,  2016,   257,   922,  5891,  1576]])\n",
      "tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257],\n",
      "        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n",
      "        [ 1464,  1807,  3619,   402,   271, 10899,  2138,   257,  7026, 15632],\n",
      "        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026, 15632,   438],\n",
      "        [ 3619,   402,   271, 10899,  2138,   257,  7026, 15632,   438,  2016],\n",
      "        [  402,   271, 10899,  2138,   257,  7026, 15632,   438,  2016,   257],\n",
      "        [  271, 10899,  2138,   257,  7026, 15632,   438,  2016,   257,   922],\n",
      "        [10899,  2138,   257,  7026, 15632,   438,  2016,   257,   922,  5891],\n",
      "        [ 2138,   257,  7026, 15632,   438,  2016,   257,   922,  5891,  1576],\n",
      "        [  257,  7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438]])\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "enc_txt = tokenizer.encode(txt)\n",
    "for i in range(0, len(enc_txt) - context, stride):\n",
    "    x = enc_txt[i:i+context]\n",
    "    y = enc_txt[i+1:i+context+1]\n",
    "    x_train.append(torch.tensor(x))\n",
    "    y_train.append(torch.tensor(y))\n",
    "x_train = torch.stack(x_train)\n",
    "y_train = torch.stack(y_train)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_train[:10])\n",
    "print(y_train[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GPTDatasetV1(raw_text, tiktoken.get_encoding(\"gpt2\"), context_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 5135\n",
      "Input shape: torch.Size([5135, 10]), Target shape: torch.Size([5135, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "x, y = dataset[:]\n",
    "print(f\"Input shape: {x.shape}, Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "Input shape: torch.Size([32, 10]), Target shape: torch.Size([32, 10])\n",
      "Batch 1:\n",
      "Input shape: torch.Size([32, 10]), Target shape: torch.Size([32, 10])\n",
      "Batch 2:\n",
      "Input shape: torch.Size([32, 10]), Target shape: torch.Size([32, 10])\n",
      "\n",
      "Total number of batches: 160\n"
     ]
    }
   ],
   "source": [
    "dataloader = dataset.load_data()\n",
    "batch_count = 0\n",
    "for x, y in dataloader:\n",
    "    print(f\"Batch {batch_count}:\")\n",
    "    print(f\"Input shape: {x.shape}, Target shape: {y.shape}\")\n",
    "    batch_count += 1\n",
    "    if batch_count == 3:  # Just showing first 3 batches as example\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 10]), Target shape: torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(f\"Input shape: {first_batch[0].shape}, Target shape: {first_batch[1].shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Example:\n",
      "Inputs:  torch.Size([10, 8]) \n",
      " tensor([[  198,   198,  1890,   262,   717,   640,   616, 21696],\n",
      "        [  438,   261, 45697, 19369,    11,   355,   345,   910],\n",
      "        [  284,   467,   257,  1310,  4295,   438,    40,  2936],\n",
      "        [  257, 17779,   339,   550, 11564,   284,  1657,    13],\n",
      "        [  339,   531, 15376,    26,   788,    11,  6427,   465],\n",
      "        [  743,   307, 41746, 12004,   262,  6473,   438,  5562],\n",
      "        [11428,   450,    67,  3299,    13,   887,   645,   438],\n",
      "        [   40,  1297,  9074,    13,   520,  5493,   523,   618],\n",
      "        [10724,   262,  6846,   338, 11428,   450,    67,  3299],\n",
      "        [   13,  3226,  1781,   314,  1422,   470,  1560,   607]])\n",
      "Targets:  torch.Size([10, 8]) \n",
      " tensor([[  198,  1890,   262,   717,   640,   616, 21696, 20136],\n",
      "        [  261, 45697, 19369,    11,   355,   345,   910,    13],\n",
      "        [  467,   257,  1310,  4295,   438,    40,  2936, 10927],\n",
      "        [17779,   339,   550, 11564,   284,  1657,    13, 24975],\n",
      "        [  531, 15376,    26,   788,    11,  6427,   465,  3211],\n",
      "        [  307, 41746, 12004,   262,  6473,   438,  5562,   314],\n",
      "        [  450,    67,  3299,    13,   887,   645,   438,  1640],\n",
      "        [ 1297,  9074,    13,   520,  5493,   523,   618,   673],\n",
      "        [  262,  6846,   338, 11428,   450,    67,  3299,    13],\n",
      "        [ 3226,  1781,   314,  1422,   470,  1560,   607,  4808]])\n",
      "Number of batches: 256\n"
     ]
    }
   ],
   "source": [
    "dataset = GPTDatasetV1(raw_text, tiktoken.get_encoding(\"gpt2\"), context_size=8, stride=2)\n",
    "dataloader = dataset.load_data(batch_size=10)\n",
    "data_iter = iter(dataloader)\n",
    "x, y = next(data_iter)\n",
    "print(\"Batch Example:\")\n",
    "print(\"Inputs: \", x.shape, \"\\n\", x)\n",
    "print(\"Targets: \", y.shape, \"\\n\", y)\n",
    "print(\"Number of batches:\", len(dataloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 32])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 32\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "print(embedding_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([10, 8])\n",
      "Embedded shape: torch.Size([10, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of data\n",
    "data_iter = iter(dataloader)\n",
    "x, _ = next(data_iter)\n",
    "\n",
    "# Embed the batch\n",
    "embedded = embedding_layer(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Embedded shape:\", embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.8631e+00,  8.4178e-01,  9.7228e-01,  ..., -9.6654e-01,\n",
      "           3.2825e-02, -9.1771e-01],\n",
      "         [-6.6487e-01,  1.2974e+00,  2.1441e-01,  ..., -5.7817e-01,\n",
      "          -1.1918e+00,  1.9304e-01],\n",
      "         [ 7.2200e-01,  1.7812e+00,  1.5288e+00,  ...,  1.4561e-01,\n",
      "           1.4557e+00, -8.1448e-01],\n",
      "         ...,\n",
      "         [-9.7156e-03,  1.1120e+00, -5.2111e-01,  ...,  1.6579e+00,\n",
      "           2.5478e-01, -9.6888e-01],\n",
      "         [ 6.7424e-01,  5.6093e-01, -8.5805e-01,  ...,  1.1551e-01,\n",
      "          -1.0838e+00, -5.2731e-01],\n",
      "         [ 1.8631e+00,  8.4178e-01,  9.7228e-01,  ..., -9.6654e-01,\n",
      "           3.2825e-02, -9.1771e-01]],\n",
      "\n",
      "        [[-1.7048e-01,  2.0849e-01,  7.1294e-01,  ..., -4.4171e-02,\n",
      "          -1.2940e+00,  2.9590e-01],\n",
      "         [-5.6068e-01, -4.5873e-01, -4.6548e-01,  ...,  2.1205e-03,\n",
      "           1.4232e+00,  1.8232e+00],\n",
      "         [ 2.4071e-01,  9.0306e-01, -1.7855e+00,  ..., -1.0483e+00,\n",
      "           2.2495e-01,  9.9595e-01],\n",
      "         ...,\n",
      "         [ 2.1060e+00,  1.4851e+00, -6.6253e-01,  ...,  2.0784e+00,\n",
      "          -6.7237e-01,  1.1881e+00],\n",
      "         [ 1.9750e+00,  2.7249e-01, -4.1028e-01,  ..., -1.3142e-01,\n",
      "          -8.4127e-01, -1.2459e+00],\n",
      "         [-6.7899e-01, -6.0342e-01, -5.3408e-01,  ..., -1.0333e+00,\n",
      "          -3.7686e-01, -4.6664e-02]],\n",
      "\n",
      "        [[-1.2643e+00,  3.6925e-01,  6.4198e-01,  ..., -1.7200e+00,\n",
      "          -2.6745e-01, -1.0252e+00],\n",
      "         [-1.1524e+00,  1.3701e-01, -3.3668e-01,  ...,  5.7135e-01,\n",
      "           6.6670e-01,  7.5298e-01],\n",
      "         [-2.2771e-01,  1.0598e+00,  3.6786e-01,  ..., -1.0316e+00,\n",
      "          -9.9197e-01,  1.4855e-01],\n",
      "         ...,\n",
      "         [ 2.4425e+00, -1.1472e+00, -1.3964e-01,  ..., -4.6599e-01,\n",
      "           2.8902e+00, -8.1455e-01],\n",
      "         [ 3.7955e-01, -5.6397e-01, -5.5596e-01,  ..., -2.0458e-01,\n",
      "           1.8209e+00, -1.0874e+00],\n",
      "         [-1.1524e+00,  1.3701e-01, -3.3668e-01,  ...,  5.7135e-01,\n",
      "           6.6670e-01,  7.5298e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0712e+00,  1.0689e+00,  5.2870e-01,  ..., -1.1919e+00,\n",
      "           5.5271e-01, -7.2778e-01],\n",
      "         [ 6.2323e-01, -5.0526e-01,  1.2434e+00,  ...,  6.9443e-01,\n",
      "           1.9020e-01,  2.0555e+00],\n",
      "         [ 2.7201e-01,  9.1805e-02, -1.6268e+00,  ...,  2.0675e+00,\n",
      "          -8.8080e-01, -1.1435e+00],\n",
      "         ...,\n",
      "         [-3.8438e-01, -1.1669e-01,  2.2763e+00,  ...,  1.0262e+00,\n",
      "           8.7016e-01,  1.2245e-01],\n",
      "         [-2.8509e-01,  4.4695e-01,  3.2697e-01,  ...,  7.8829e-02,\n",
      "          -1.9623e+00, -2.0975e-01],\n",
      "         [ 1.5491e+00,  4.1330e-01,  4.4555e-01,  ..., -8.9763e-01,\n",
      "          -3.0346e-01,  1.4416e-01]],\n",
      "\n",
      "        [[-6.7899e-01, -6.0342e-01, -5.3408e-01,  ..., -1.0333e+00,\n",
      "          -3.7686e-01, -4.6664e-02],\n",
      "         [-4.8108e-01, -2.4320e+00, -1.1795e+00,  ...,  8.1677e-01,\n",
      "           1.0703e+00, -2.2431e+00],\n",
      "         [ 7.6918e-01, -6.4586e-01, -1.3644e+00,  ..., -3.3334e-02,\n",
      "           1.4884e-01, -2.1335e-01],\n",
      "         ...,\n",
      "         [ 1.5152e+00, -2.4270e+00,  1.5948e-02,  ...,  4.5186e-01,\n",
      "          -2.8995e-01,  4.7473e-02],\n",
      "         [-1.1275e+00,  2.1228e+00,  1.3848e+00,  ...,  4.2627e-01,\n",
      "           9.9436e-01,  1.3864e+00],\n",
      "         [-7.1732e-01, -6.2480e-01,  6.7405e-01,  ...,  5.2478e-01,\n",
      "           1.1931e-01,  1.7210e+00]],\n",
      "\n",
      "        [[ 7.8915e-01, -4.1291e-01,  3.6147e-01,  ...,  5.1049e-01,\n",
      "          -4.8981e-01,  5.2576e-01],\n",
      "         [-1.5713e+00,  3.1449e-02,  2.4043e+00,  ...,  1.4977e+00,\n",
      "          -9.1942e-01, -6.4194e-01],\n",
      "         [ 2.5832e-01, -1.4371e+00,  1.3129e+00,  ..., -1.0387e+00,\n",
      "           5.6020e-01,  1.1719e+00],\n",
      "         ...,\n",
      "         [ 1.1627e+00,  4.5701e-01,  4.6713e-01,  ...,  9.2531e-01,\n",
      "          -4.2338e-02, -3.3832e-01],\n",
      "         [ 3.9282e-02,  9.2569e-01,  8.8795e-01,  ..., -6.2729e-01,\n",
      "           1.0434e+00, -1.1263e+00],\n",
      "         [ 4.7636e-01,  9.5659e-01,  1.2605e+00,  ..., -7.2151e-01,\n",
      "           1.2951e-01,  3.6251e-01]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.8631,  0.8418,  0.9723, -0.5850,  0.3627, -0.2022,  0.3914, -0.1290,\n",
      "         0.8355,  1.2098,  1.6261, -0.4519, -0.0081,  0.9334,  2.2611,  0.0725,\n",
      "         0.4023, -0.1037, -0.5099, -0.8252,  0.0823,  0.5279, -0.9744,  0.0575,\n",
      "        -1.2271,  1.2841,  0.9220,  0.3859,  0.5539, -0.9665,  0.0328, -0.9177],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(embedded[0][0])\n",
    "print(embedded[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# dataset and dataloader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, context_size=4, stride=1):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        enc_txt = tokenizer.encode(text)\n",
    "        for i in range(0, len(enc_txt) - context_size, stride):\n",
    "            x = enc_txt[i:i+context_size]\n",
    "            y = enc_txt[i+1:i+context_size+1]\n",
    "            self.x.append(torch.tensor(x))\n",
    "            self.y.append(torch.tensor(y))\n",
    "        self.x = torch.stack(self.x)\n",
    "        self.y = torch.stack(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def load_data(self, batch_size=32, shuffle=True):\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle, drop_last=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = tokenizer.n_vocab\n",
    "context_size = 4\n",
    "embedding_dim = 256\n",
    "# layers\n",
    "token_embedding_layer = nn.Embedding(vocab_size, embedding_dim) \n",
    "positional_embedding_layer = nn.Embedding(context_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "dataset = GPTDatasetV1(raw_text, tokenizer, context_size=context_size, stride=1)\n",
    "# dataloader\n",
    "dataloader = dataset.load_data(batch_size=8)\n",
    "data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch\n",
    "x, y = next(data_iter)\n",
    "# embed batch\n",
    "token_embedded = token_embedding_layer(x) # [B, T, C]\n",
    "positional_embedded = positional_embedding_layer(torch.arange(context_size)) # [T, C]\n",
    "embedded = token_embedded + positional_embedded # [B, T, C]\n",
    "# TODO: review broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
