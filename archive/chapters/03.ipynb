{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "tensor([[-1.8969, -0.5555,  0.4984],\n",
      "        [-0.7669,  1.3222,  0.1500],\n",
      "        [ 0.0465,  1.4264, -1.4913],\n",
      "        [ 1.5451, -1.1425, -1.7921],\n",
      "        [-0.3205, -1.4035,  0.1314]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 3)\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "tensor([[9.0453e-01, 3.1417e-02, 2.7969e-03, 5.8445e-04, 6.0671e-02],\n",
      "        [1.2210e-01, 5.8334e-01, 2.8047e-01, 2.8441e-03, 1.1242e-02],\n",
      "        [2.4856e-03, 6.4135e-02, 8.9357e-01, 3.8434e-02, 1.3792e-03],\n",
      "        [4.1108e-05, 5.1471e-05, 3.0419e-03, 9.9448e-01, 2.3884e-03],\n",
      "        [2.8385e-01, 1.3533e-02, 7.2611e-03, 1.5887e-01, 5.3648e-01]])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = [] \n",
    "for i in range(x.shape[0]):  \n",
    "    query = x[i]\n",
    "    scores = torch.empty(x.shape[0])\n",
    "    for j, j_x in enumerate(x):\n",
    "        scores[j] = torch.dot(query, j_x)\n",
    "    attention_scores.append(scores)\n",
    "attention_scores = torch.stack(attention_scores)\n",
    "attention_scores = attention_scores.softmax(dim=1)\n",
    "print(attention_scores.shape)\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(attention_scores.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "tensor([[-1.7583, -0.5428,  0.4583],\n",
      "        [-0.6652,  1.0845, -0.2735],\n",
      "        [ 0.0466,  1.3122, -1.3904],\n",
      "        [ 1.5359, -1.1352, -1.7864],\n",
      "        [-0.4750, -1.0639, -0.0816]])\n"
     ]
    }
   ],
   "source": [
    "context_vectors = attention_scores @ x\n",
    "print(context_vectors.shape)\n",
    "print(context_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        tokens = x[0]\n",
    "        positions = x[1]\n",
    "        return self.token_embedding(tokens) + self.position_embedding(positions)\n",
    "\n",
    "class AttentionLayerV1(nn.Module):\n",
    "    def __init__(self, embed_size, out_dim):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.q = nn.Parameter(torch.randn(embed_size, out_dim))\n",
    "        self.k = nn.Parameter(torch.randn(embed_size, out_dim))\n",
    "        self.v = nn.Parameter(torch.randn(embed_size, out_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = x @ self.q\n",
    "        k = x @ self.k\n",
    "        v = x @ self.v\n",
    "        attention_scores = q @ k.T \n",
    "        attention_weights = torch.softmax(attention_scores / k.shape[-1] ** 0.5, dim=1)\n",
    "        context_vectors = attention_weights @ v\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "tensor([[1.3423, 4.5167],\n",
      "        [1.7081, 5.3384],\n",
      "        [1.1703, 4.5721],\n",
      "        [0.2930, 2.5763],\n",
      "        [0.9384, 3.6728]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "T = 5\n",
    "C = 3\n",
    "\n",
    "input = torch.randn(T, C)\n",
    "attention_layer = AttentionLayerV1(embed_size=C, out_dim=2)\n",
    "output = attention_layer(input)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayerV2(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.K  = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.V = nn.Linear(d_in, d_out, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        attention_scores = q @ k.T\n",
    "        attention_weights = torch.softmax(attention_scores / k.shape[-1] ** 0.5, dim=1)\n",
    "        context_vectors = attention_weights @ v\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "tensor([[0.2427, 0.1754],\n",
      "        [0.3014, 0.1309],\n",
      "        [0.2375, 0.2434],\n",
      "        [0.3232, 0.1164],\n",
      "        [0.3114, 0.1015]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "T = 5\n",
    "C = 3\n",
    "\n",
    "inputs = torch.randn(T, C)\n",
    "attention_layer = AttentionLayerV2(d_in=C, d_out=2)\n",
    "output = attention_layer(inputs)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionLayerV3(nn.Module):\n",
    "    def __init__(self, d_in, d_out, masked=False):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.K  = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.V = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.masked = masked\n",
    "    def forward(self, x):\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        attention_scores = q @ k.T\n",
    "        attention_weights = torch.softmax(attention_scores / k.shape[-1] ** 0.5, dim=1)\n",
    "        if self.masked:\n",
    "            mask = torch.tril(torch.ones_like(attention_weights))\n",
    "            attention_weights = attention_weights * mask\n",
    "            print(attention_weights)\n",
    "\n",
    "        context_vectors = attention_weights @ v\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2190, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1708, 0.1828, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0100, 0.0285, 0.1171, 0.0000, 0.0000],\n",
      "        [0.2968, 0.2417, 0.2072, 0.0859, 0.0000],\n",
      "        [0.2077, 0.2047, 0.2038, 0.1863, 0.1975]], grad_fn=<MulBackward0>)\n",
      "torch.Size([5, 2])\n",
      "tensor([[ 0.1010,  0.1206],\n",
      "        [ 0.1581,  0.2144],\n",
      "        [-0.1219,  0.2185],\n",
      "        [ 0.0229,  0.5805],\n",
      "        [-0.0137,  0.3659]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "T = 5\n",
    "C = 3\n",
    "\n",
    "inputs = torch.randn(T, C)\n",
    "attention_layer = AttentionLayerV3(d_in=C, d_out=2, masked=True)\n",
    "output = attention_layer(inputs)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_len, embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        tok_embed = self.token_embedding(x)\n",
    "        pos_embed = self.position_embedding(torch.arange(len(x)))\n",
    "        return tok_embed + pos_embed\n",
    "\n",
    "class AttentionLayerV4(nn.Module):\n",
    "    def __init__(self, d_in, d_out, masked=False):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.K  = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.V = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.masked = masked\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        attention_scores = q @ k.T\n",
    "        if self.masked:\n",
    "            mask = torch.triu(torch.ones_like(attention_scores), diagonal=1)\n",
    "            attention_scores = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores / k.shape[-1] ** 0.5, dim=1)\n",
    "        return attention_weights @ v\n",
    "    \n",
    "class NetworkV1(nn.Module):\n",
    "    def __init__(self, tokenizer, vocab_size, embed_size, max_len, masked=False):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding_layer = EmbeddingLayer(vocab_size, embed_size, max_len)\n",
    "        self.attention_layer = AttentionLayerV4(embed_size, embed_size, masked)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding_layer(torch.tensor(self.tokenizer.encode(x)))\n",
    "        attention_vectors = self.attention_layer(embedded)\n",
    "        return attention_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 3\n",
    "context_size = 50\n",
    "\n",
    "network = NetworkV1(tokenizer, vocab_size, embedding_dim, context_size, masked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 3])\n",
      "tensor([[ 0.0787,  0.1318,  0.5671],\n",
      "        [-0.1120,  0.5600,  0.6286],\n",
      "        [ 0.0086,  0.3003,  1.1442],\n",
      "        [-0.1068,  0.5123,  0.0449],\n",
      "        [-0.1089,  0.3998,  0.2162],\n",
      "        [ 0.0547,  0.1458,  0.5653],\n",
      "        [ 0.0313,  0.0811,  0.3636],\n",
      "        [ 0.0821, -0.0616,  0.1928],\n",
      "        [ 0.0978, -0.0822, -0.7275],\n",
      "        [ 0.0203, -0.0455,  0.1381],\n",
      "        [ 0.0449, -0.0515,  0.1032],\n",
      "        [ 0.0344, -0.0356, -0.4158],\n",
      "        [-0.0138,  0.1253, -1.1324]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = \"Hello, world. My name is Max. I love Murphy.\"\n",
    "output = network(input)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_len, embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        tok_embed = self.token_embedding(x)\n",
    "        pos_embed = self.position_embedding(torch.arange(x.shape[1]))\n",
    "        return tok_embed + pos_embed\n",
    "\n",
    "class AttentionLayerV5(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout, masked=False):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.K  = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.V = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.masked = masked\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "        if self.masked:\n",
    "            mask = torch.triu(torch.ones_like(attention_scores), diagonal=1)\n",
    "            attention_scores = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores / k.shape[-1] ** 0.5, dim=1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        return attention_weights @ v\n",
    "    \n",
    "class NetworkV2(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_len, dropout=0.1, masked=False):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = EmbeddingLayer(vocab_size, embed_size, max_len)\n",
    "        self.attention_layer = AttentionLayerV5(embed_size, embed_size, dropout, masked)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding_layer(x)\n",
    "        attention_vectors = self.attention_layer(embedded)\n",
    "        return attention_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NetworkV2(vocab_size, embedding_dim, context_size, dropout=0.1, masked=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 13, 3])\n",
      "tensor([[[-0.0286,  0.0067,  0.0113],\n",
      "         [-0.0365,  0.0081,  0.0166],\n",
      "         [-0.2158,  0.0439,  0.1001],\n",
      "         [-0.1640,  0.0309,  0.0587],\n",
      "         [-0.1412,  0.0305,  0.0167],\n",
      "         [ 0.1642, -0.0353, -0.2198],\n",
      "         [ 0.2214, -0.0447, -0.2518],\n",
      "         [ 0.1846, -0.0421, -0.2585],\n",
      "         [ 0.3294, -0.0770, -0.3810],\n",
      "         [ 0.2523, -0.0461, -0.0219],\n",
      "         [ 0.4665, -0.0565,  0.4649],\n",
      "         [-0.4479,  0.0916,  0.3518],\n",
      "         [ 1.4957, -0.3089, -0.7081]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.0365,  0.0081,  0.0166],\n",
      "         [-0.2158,  0.0439,  0.1001],\n",
      "         [-0.1640,  0.0309,  0.0587],\n",
      "         [-0.1412,  0.0305,  0.0167],\n",
      "         [ 0.1642, -0.0353, -0.2198],\n",
      "         [ 0.1669, -0.0358, -0.1990],\n",
      "         [ 0.2308, -0.0529, -0.3341],\n",
      "         [ 0.2033, -0.0507, -0.3135],\n",
      "         [ 0.0541, -0.0048,  0.0841],\n",
      "         [ 0.2890, -0.0240,  0.5829],\n",
      "         [-0.3689,  0.0560,  0.2129],\n",
      "         [ 1.5016, -0.3081, -0.5365]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = \"Hello, world. My name is Max. I love Murphy.\"\n",
    "tokenized_input = torch.tensor(tokenizer.encode(input))\n",
    "output = network(torch.stack([tokenized_input, tokenized_input]))\n",
    "\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "context_size = 5\n",
    "batch_size = 2\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "embed_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset and dataloader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, context_size):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        enc_txt = tokenizer.encode(text)\n",
    "        for i in range(0, len(enc_txt) - context_size):\n",
    "            x = enc_txt[i:i+context_size]\n",
    "            y = enc_txt[i+1:i+context_size+1]\n",
    "            self.x.append(torch.tensor(x))\n",
    "            self.y.append(torch.tensor(y))\n",
    "        self.x = torch.stack(self.x)\n",
    "        self.y = torch.stack(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def load_data(self, batch_size, shuffle=True):\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle, drop_last=True, num_workers=0)\n",
    "\n",
    "dataset = GPTDatasetV1(raw_text, tokenizer, context_size)\n",
    "dataloader = dataset.load_data(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 3])\n",
      "tensor([[[-0.1015,  0.0978,  0.0220],\n",
      "         [-0.2829,  0.0887,  0.1696],\n",
      "         [-0.6552,  0.4291,  0.3331],\n",
      "         [-0.1645,  0.0239,  0.3725],\n",
      "         [ 0.1294, -0.8983,  1.2846]],\n",
      "\n",
      "        [[-0.1975,  0.1803,  0.1630],\n",
      "         [-0.1187,  0.1084,  0.0980],\n",
      "         [ 0.0194, -0.4354,  0.8311],\n",
      "         [ 0.4346, -0.3015,  0.3292],\n",
      "         [ 0.4168, -1.7130,  2.3964]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "x, y = next(data_iter)\n",
    "network = NetworkV2(vocab_size, embed_size, context_size, dropout=0.1, masked=True)\n",
    "output = network(x)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, n_heads, dropout, max_len):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_out // n_heads\n",
    "        self.q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.k = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.v = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(max_len, max_len)), diagonal=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        Q = self.q(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        K = self.k(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        V = self.v(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        attention_scores = Q @ K.transpose(-2, -1)\n",
    "        mask = self.mask[:T, :T]\n",
    "        attention_scores = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "        attention_scores = attention_scores / (self.head_dim ** 0.5)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context_vectors = attention_weights @ V\n",
    "        context_vectors = context_vectors.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_len, embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        tok_embed = self.token_embedding(x)\n",
    "        pos_embed = self.position_embedding(torch.arange(x.shape[1]))\n",
    "        return tok_embed + pos_embed\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, n_heads, dropout, masked, max_len):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_out // n_heads\n",
    "        self.masked = masked\n",
    "        self.q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.k = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.v = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(max_len, max_len), diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        Q = self.q(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        attention_scores = Q @ K.transpose(-2, -1)\n",
    "        if self.masked:\n",
    "            mask = self.mask[:T, :T]\n",
    "            attention_scores = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores / (self.head_dim ** 0.5), dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context_vectors = attention_weights @ V\n",
    "        context_vectors = context_vectors.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(context_vectors)\n",
    "    \n",
    "class NetworkV3(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_len, n_heads, dropout, masked):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = EmbeddingLayer(vocab_size, embed_size, max_len)\n",
    "        self.attention_layer = MultiHeadAttention(embed_size, embed_size, n_heads, dropout, masked, max_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding_layer(x)\n",
    "        attention_vectors = self.attention_layer(embedded)\n",
    "        return attention_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 32])\n",
      "tensor([[[ 2.8852e-01, -5.6105e-01,  1.1510e+00, -4.1363e-01,  4.7154e-01,\n",
      "           3.3004e-01, -1.0168e+00, -3.6955e-01, -3.0763e-01, -5.3831e-01,\n",
      "           1.2515e+00,  2.9894e-01, -8.6049e-01,  1.1644e+00,  2.5153e-04,\n",
      "          -4.2355e-02, -3.7688e-01, -2.7146e-01,  3.1471e-01,  1.0502e+00,\n",
      "          -1.1227e+00,  3.5266e-01, -1.9511e-01, -3.6848e-01, -1.9411e-01,\n",
      "           7.1185e-01, -1.3753e-02, -6.1759e-01, -9.5560e-01,  9.1207e-01,\n",
      "          -3.2594e-01,  4.9692e-01],\n",
      "         [ 6.2506e-01,  3.6419e-01,  8.1589e-01, -8.4784e-02,  6.6278e-01,\n",
      "           7.9160e-01, -3.1044e-01,  3.0327e-01, -1.0714e+00,  8.7699e-03,\n",
      "           6.7571e-01, -3.4535e-01, -5.1409e-01,  4.5898e-01, -7.5577e-02,\n",
      "           5.2646e-01,  3.6184e-01, -3.3804e-01,  5.4978e-01, -1.1970e-01,\n",
      "           2.5241e-01,  7.9726e-02, -9.6158e-01, -1.2332e+00, -7.9589e-01,\n",
      "           3.2263e-01, -3.9863e-02, -4.6020e-01, -1.0783e+00,  5.0953e-01,\n",
      "          -5.3599e-01,  2.6431e-01],\n",
      "         [ 1.2864e-01, -3.6825e-01,  1.8047e-01, -5.4537e-01,  1.4882e-01,\n",
      "           6.5625e-02, -4.0997e-01, -2.3092e-01, -1.7162e-01, -5.7909e-02,\n",
      "           4.9040e-01,  1.0636e-01, -4.5008e-01,  5.5711e-01, -6.9436e-02,\n",
      "          -5.2660e-01,  1.1989e-01, -3.5759e-01,  8.0675e-02,  5.1677e-01,\n",
      "          -2.5381e-01,  1.0974e-01,  2.4523e-01, -2.4198e-01,  1.4872e-02,\n",
      "           3.0428e-01, -2.0164e-01,  1.7163e-01, -3.5679e-01,  5.7290e-02,\n",
      "          -5.1801e-02, -7.4538e-02],\n",
      "         [ 4.1164e-01, -6.5252e-03, -1.4191e-01, -3.6551e-01, -2.0317e-01,\n",
      "           4.6789e-01, -1.0056e-01,  3.3277e-02, -5.1090e-02,  1.9473e-01,\n",
      "           1.2519e-01, -3.3631e-02, -6.1760e-01,  3.0999e-01, -2.0482e-02,\n",
      "           4.4332e-02,  1.6162e-01, -2.7280e-01, -2.2907e-01, -8.5812e-02,\n",
      "           9.2225e-02,  6.1215e-02, -4.8650e-01, -3.6073e-01,  1.7959e-01,\n",
      "           1.3719e-01, -4.9476e-01, -2.4794e-01, -4.4731e-01, -2.1376e-03,\n",
      "           1.9947e-01,  1.4672e-02],\n",
      "         [ 4.0400e-01,  2.5199e-02,  3.6899e-02, -3.5699e-01, -4.1003e-02,\n",
      "           4.1777e-01, -2.5725e-01, -1.5708e-01, -6.1860e-02,  1.0790e-01,\n",
      "           3.1821e-01,  5.8306e-02, -5.1661e-01,  4.9584e-01, -2.6062e-02,\n",
      "           1.0929e-01,  2.9278e-02, -3.1445e-01, -1.1055e-01, -6.8457e-02,\n",
      "          -9.6632e-02, -1.7877e-03, -3.5980e-01, -3.6394e-01, -8.2437e-02,\n",
      "           1.1884e-01, -4.6769e-01, -2.4950e-01, -5.2819e-01,  1.6131e-01,\n",
      "           1.4436e-01,  1.1649e-01]],\n",
      "\n",
      "        [[ 3.4891e-01, -1.8660e-01,  1.3378e-01, -3.8382e-01,  7.6946e-02,\n",
      "           5.5006e-01, -2.3344e-01,  9.1261e-03, -5.3729e-01, -2.5666e-01,\n",
      "           9.7417e-02,  8.6108e-02, -6.0221e-01,  3.8737e-01, -1.1993e-01,\n",
      "          -9.9746e-02,  1.2065e-01, -4.9450e-01, -1.5125e-01,  2.7645e-01,\n",
      "          -5.2453e-01,  3.6633e-01, -1.4927e-01,  1.0091e-02,  2.9205e-01,\n",
      "           7.1641e-01, -6.2594e-01, -2.9324e-01, -7.8569e-01,  3.5022e-01,\n",
      "           1.7396e-01,  6.0190e-01],\n",
      "         [ 8.1028e-01, -4.4717e-01,  1.4611e-01, -6.2503e-01, -2.7803e-01,\n",
      "           1.1120e+00, -2.0200e-01, -3.7623e-02, -6.5631e-01, -3.5883e-01,\n",
      "          -6.6387e-02,  1.6268e-01, -2.8598e-01,  5.4872e-01, -5.0803e-01,\n",
      "           3.2162e-01,  3.4512e-01, -6.7175e-01,  2.5935e-01,  1.0980e-01,\n",
      "          -3.2163e-01,  8.2676e-01, -2.8544e-01,  6.1836e-01,  6.4344e-01,\n",
      "           6.7959e-02, -5.6180e-01, -3.5075e-01, -2.0530e-01,  2.3670e-01,\n",
      "           1.3373e-01,  6.8331e-01],\n",
      "         [ 3.3142e-01, -2.8686e-01, -1.9949e-03, -2.1588e-01,  8.9232e-02,\n",
      "           3.1915e-01, -2.5226e-01,  1.3694e-01, -7.3936e-01, -1.7979e-01,\n",
      "          -4.8043e-02,  1.0193e-01, -3.5436e-01,  3.5713e-01,  1.8007e-03,\n",
      "          -2.7274e-01,  1.0940e-01, -4.5506e-01, -2.0497e-02, -1.4760e-02,\n",
      "          -4.4526e-01,  3.2586e-01,  2.3032e-01,  1.5540e-01,  1.2258e-01,\n",
      "           6.8911e-01, -4.5101e-01, -2.2387e-01, -8.8585e-01,  3.4433e-01,\n",
      "           5.8531e-02,  1.9928e-01],\n",
      "         [ 1.7447e-01, -2.2526e-02, -1.6517e-01, -4.1422e-01,  6.8567e-02,\n",
      "           4.1136e-02, -1.2009e-01,  6.4066e-02, -1.2442e-01,  3.4252e-01,\n",
      "           2.6689e-01,  1.2195e-01, -2.2289e-01,  2.2699e-01, -1.3536e-01,\n",
      "          -1.3375e-02,  2.1023e-01,  2.2857e-02,  1.6098e-01,  1.6943e-01,\n",
      "          -1.9006e-01,  1.1910e-01, -4.2520e-01, -7.4124e-02,  7.3936e-02,\n",
      "           6.5490e-02, -1.7325e-01, -8.9506e-02, -5.2944e-01, -1.3657e-01,\n",
      "           9.8591e-02, -1.9990e-01],\n",
      "         [ 1.5809e-01,  6.9293e-02, -1.1692e-01, -5.3579e-01,  2.0681e-01,\n",
      "           2.3904e-01,  8.5197e-02,  1.0408e-01,  3.6435e-01,  3.8386e-01,\n",
      "           7.5002e-02,  2.6572e-01, -2.8044e-01,  3.4748e-01, -1.4575e-01,\n",
      "           6.6946e-02,  1.2543e-01, -8.4021e-02, -9.2655e-02, -3.1551e-01,\n",
      "          -2.2159e-03, -2.1813e-01, -2.0328e-01, -2.2737e-01, -6.0622e-04,\n",
      "           2.4047e-01, -3.1586e-01, -5.8486e-02, -5.2386e-01, -1.5481e-01,\n",
      "           8.2103e-02,  2.0370e-03]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "embed_size = 32\n",
    "max_len = 10\n",
    "n_heads = 2\n",
    "dropout = 0.1\n",
    "masked = True\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "x, y = next(data_iter)\n",
    "network = NetworkV3(vocab_size, embed_size, max_len, n_heads, dropout, masked)\n",
    "output = network(x)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
