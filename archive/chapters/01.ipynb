{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "print(f\"Number of characters: {len(raw_text)}\")\n",
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world! This is a test.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = re.split(r'(\\s)', text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'world!',\n",
       " ' ',\n",
       " 'This',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'test',\n",
       " '.',\n",
       " '']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world!', 'This', 'is', 'a', 'test', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [token for token in result if token.strip()]\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '!', 'Is', 'this', '--', 'is', 'a', 'test', '?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello, world! Is this-- is a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [token.strip() for token in result if token.strip()]\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    result = [token.strip() for token in result if token.strip()]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 4690\n",
      "['me', 'forward', 'But', 'inflexible', 'his', 'Yes', 'been', 'in', 'had', 'had']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = tokenize(raw_text)\n",
    "print(f\"Number of tokens: {len(preprocessed)}\")\n",
    "print(random.sample(preprocessed, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(preprocessed))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!', 0),\n",
       " ('\"', 1),\n",
       " (\"'\", 2),\n",
       " ('(', 3),\n",
       " (')', 4),\n",
       " (',', 5),\n",
       " ('--', 6),\n",
       " ('.', 7),\n",
       " (':', 8),\n",
       " (';', 9)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctoi = { c:i for i, c in enumerate(vocab)}\n",
    "list(ctoi.items())[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '!'),\n",
       " (1, '\"'),\n",
       " (2, \"'\"),\n",
       " (3, '('),\n",
       " (4, ')'),\n",
       " (5, ','),\n",
       " (6, '--'),\n",
       " (7, '.'),\n",
       " (8, ':'),\n",
       " (9, ';'),\n",
       " (10, '?'),\n",
       " (11, 'A'),\n",
       " (12, 'Ah'),\n",
       " (13, 'Among'),\n",
       " (14, 'And'),\n",
       " (15, 'Are'),\n",
       " (16, 'Arrt'),\n",
       " (17, 'As'),\n",
       " (18, 'At'),\n",
       " (19, 'Be'),\n",
       " (20, 'Begin'),\n",
       " (21, 'Burlington'),\n",
       " (22, 'But'),\n",
       " (23, 'By'),\n",
       " (24, 'Carlo'),\n",
       " (25, 'Chicago'),\n",
       " (26, 'Claude'),\n",
       " (27, 'Come'),\n",
       " (28, 'Croft'),\n",
       " (29, 'Destroyed')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itoc = {i:c for c, i in ctoi.items()}\n",
    "list(itoc.items())[:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    def __init__(self, vocab=None):\n",
    "        print(\"TokenizerV1 initialized\")\n",
    "        if vocab is None:\n",
    "            self.vocab = None\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "    \n",
    "    def build_vocab(self, type='word', corpus=None):\n",
    "        if corpus is None:\n",
    "            corpus = self.corpus\n",
    "        if type == 'word':\n",
    "            result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', corpus)\n",
    "        elif type == 'char':\n",
    "            result = list(corpus)\n",
    "        vocab = sorted(set(token.strip() for token in result if token.strip()))\n",
    "        self.vocab = vocab\n",
    "        self.ctoi = { c:i for i, c in enumerate(vocab)}\n",
    "        self.itoc = {i:c for c, i in self.ctoi.items()}\n",
    "        return vocab\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        cleaned_tokens = [token.strip() for token in tokens if token.strip()]\n",
    "        result = [self.ctoi[token] for token in cleaned_tokens if token in self.vocab]\n",
    "        return result\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = \" \".join([self.itoc[token] for token in tokens])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenizerV1 initialized\n",
      "Number of tokens: 1130\n",
      "['so', 'much', 'grace', 'To', 'part', 'then', 'hour', 'deerhound', 'waves', 'suddenly']\n",
      "['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed']\n",
      "1130\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenizerV1()\n",
    "vocab = tokenizer.build_vocab(type='word', corpus=raw_text)\n",
    "print(f\"Number of tokens: {len(vocab)}\")\n",
    "print(random.sample(vocab, 10))\n",
    "print(vocab[:30])\n",
    "print(len(tokenizer.itoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "It' s the last he painted, you know, Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "test= tokenizer.encode(\"It's the last he painted, you know, Mrs. Gisburn said with pardonable pride.\")\n",
    "print(test)\n",
    "print(tokenizer.decode(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV2:\n",
    "    def __init__(self, vocab=None, corpus=None):\n",
    "        print(\"TokenizerV2 initialized\")\n",
    "        self.vocab = vocab\n",
    "        if not vocab and corpus:\n",
    "            self.build_vocab(corpus=corpus, type='word')\n",
    "    \n",
    "    def build_vocab(self, corpus, type='word'):\n",
    "        if type == 'word':\n",
    "            result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', corpus)\n",
    "            self.vocab_type = 'word'\n",
    "        elif type == 'char':\n",
    "            result = list(corpus)\n",
    "            self.vocab_type = 'char'\n",
    "        vocab = sorted(set(token.strip() for token in result if token.strip()))\n",
    "        vocab.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "        self.vocab = vocab\n",
    "        self.ctoi = { c:i for i, c in enumerate(self.vocab)}\n",
    "        self.itoc = {i:c for c, i in self.ctoi.items()}\n",
    "        return vocab\n",
    "\n",
    "    def encode(self, text):\n",
    "        if self.vocab_type == 'word':\n",
    "            tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        elif self.vocab_type == 'char':\n",
    "            tokens = list(text)\n",
    "        cleaned_tokens = [token.strip() for token in tokens if token.strip()]\n",
    "        result = [self.ctoi[token] if token in self.vocab else self.ctoi[\"<|unk|>\"] for token in cleaned_tokens]\n",
    "        return result\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = \" \".join([self.itoc[token] for token in tokens])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenizerV2 initialized\n",
      "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "It' s the last he painted, you know, Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenizerV2(corpus=raw_text)\n",
    "vocab = tokenizer.build_vocab(type='word', corpus=raw_text)\n",
    "tokens = tokenizer.encode(\"It's the last he painted, you know, Mrs. Gisburn said with pardonable pride.\")\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 1131, 0, 97, 584, 115, 1131, 7, 1131, 5, 1126, 5, 67, 7, 38]\n",
      "<|unk|>, <|unk|>! This is a <|unk|>. <|unk|>, you, Mrs. Gisburn\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"Hello, world! This is a test. Painted, you, Mrs. Gisburn\")\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 44, 149, 1003, 57, 38, 818, 115, 256, 486, 6, 1002, 115, 500, 435, 392, 6, 908, 585, 1077, 709, 508, 961, 1016, 663, 1016, 535, 987, 5, 568, 988, 538, 722, 549, 496, 5, 533, 514, 370, 549, 748, 5, 661, 115, 841, 1102, 5, 157, 397, 547, 568, 115, 1066, 727, 988, 84, 7, 3, 99, 53, 818, 1003, 585, 1120, 530, 208, 85, 734, 34, 7, 4, 1, 93, 538, 722, 549, 496, 1, 6, 987, 1077, 1089, 988, 1112, 242, 585, 7, 53, 244, 535, 67, 7, 37, 100, 6, 549, 602, 25, 897, 6, 326, 549, 1042, 116, 7, 1, 73, 297, 585, 2]\n",
      "I HAD always thought Jack Gisburn rather a cheap genius -- though a good fellow enough -- so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera.( Though I rather thought it would have been Rome or Florence.)\" The height of his glory\" -- that was what the women called it. I can hear Mrs. Gideon Thwing -- his last Chicago sitter -- deploring his unaccountable abdication.\" Of course it'\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(raw_text[:500])\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 983, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terrace of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terrace of the palace.\"\n",
    "text = \" <|endoftext|> \".join([text1, text2])\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /Users/mmontgomery14/Library/Python/3.9/lib/python/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/mmontgomery14/Library/Python/3.9/lib/python/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/mmontgomery14/Library/Python/3.9/lib/python/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mmontgomery14/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mmontgomery14/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mmontgomery14/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mmontgomery14/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 558, 286, 262, 20562, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terrace of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = (text)\n",
    "tokens = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "text = (\"Akwirw ier\")\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ak', 'w', 'ir', 'w', ' ', 'ier']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [tokenizer.decode([token]) for token in tokens]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
